digraph {
	graph [size="16.65,16.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	139174893431824 [label="
 (3157, 1)" fillcolor=darkolivegreen1]
	139174865236656 [label=SigmoidBackward0]
	139174865228400 -> 139174865236656
	139174865228400 [label=AddmmBackward0]
	139174866427696 -> 139174865228400
	139174866687616 [label="output_layer.bias
 (1)" fillcolor=lightblue]
	139174866687616 -> 139174866427696
	139174866427696 [label=AccumulateGrad]
	139174866434704 -> 139174865228400
	139174866434704 [label=ReluBackward0]
	139174883249408 -> 139174866434704
	139174883249408 [label=NativeBatchNormBackward0]
	139174883249600 -> 139174883249408
	139174883249600 [label=AddmmBackward0]
	139174883250560 -> 139174883249600
	139174866687056 [label="layer4.bias
 (8)" fillcolor=lightblue]
	139174866687056 -> 139174883250560
	139174883250560 [label=AccumulateGrad]
	139174883249888 -> 139174883249600
	139174883249888 [label=ReluBackward0]
	139174883250416 -> 139174883249888
	139174883250416 [label=NativeBatchNormBackward0]
	139174883250800 -> 139174883250416
	139174883250800 [label=AddmmBackward0]
	139174883249744 -> 139174883250800
	139174879389056 [label="layer3.bias
 (16)" fillcolor=lightblue]
	139174879389056 -> 139174883249744
	139174883249744 [label=AccumulateGrad]
	139174883251040 -> 139174883250800
	139174883251040 [label=ReluBackward0]
	139174883246768 -> 139174883251040
	139174883246768 [label=NativeBatchNormBackward0]
	139174883249168 -> 139174883246768
	139174883249168 [label=AddmmBackward0]
	139174883249216 -> 139174883249168
	139176098991472 [label="layer2.bias
 (32)" fillcolor=lightblue]
	139176098991472 -> 139174883249216
	139174883249216 [label=AccumulateGrad]
	139174883246192 -> 139174883249168
	139174883246192 [label=ReluBackward0]
	139174883249360 -> 139174883246192
	139174883249360 [label=NativeBatchNormBackward0]
	139174883248784 -> 139174883249360
	139174883248784 [label=AddmmBackward0]
	139174883250272 -> 139174883248784
	139174894009344 [label="layer1.bias
 (64)" fillcolor=lightblue]
	139174894009344 -> 139174883250272
	139174883250272 [label=AccumulateGrad]
	139174883248688 -> 139174883248784
	139174883248688 [label=TBackward0]
	139174883249072 -> 139174883248688
	139174893995824 [label="layer1.weight
 (64, 29)" fillcolor=lightblue]
	139174893995824 -> 139174883249072
	139174883249072 [label=AccumulateGrad]
	139174883249312 -> 139174883249360
	139174893999104 [label="bn1.weight
 (64)" fillcolor=lightblue]
	139174893999104 -> 139174883249312
	139174883249312 [label=AccumulateGrad]
	139174883246864 -> 139174883249360
	139174894007424 [label="bn1.bias
 (64)" fillcolor=lightblue]
	139174894007424 -> 139174883246864
	139174883246864 [label=AccumulateGrad]
	139174883250032 -> 139174883249168
	139174883250032 [label=TBackward0]
	139174883249456 -> 139174883250032
	139174870726480 [label="layer2.weight
 (32, 64)" fillcolor=lightblue]
	139174870726480 -> 139174883249456
	139174883249456 [label=AccumulateGrad]
	139174883248880 -> 139174883246768
	139174879111968 [label="bn2.weight
 (32)" fillcolor=lightblue]
	139174879111968 -> 139174883248880
	139174883248880 [label=AccumulateGrad]
	139174883249024 -> 139174883246768
	139174866550080 [label="bn2.bias
 (32)" fillcolor=lightblue]
	139174866550080 -> 139174883249024
	139174883249024 [label=AccumulateGrad]
	139174883249648 -> 139174883250800
	139174883249648 [label=TBackward0]
	139174883252912 -> 139174883249648
	139174893874432 [label="layer3.weight
 (16, 32)" fillcolor=lightblue]
	139174893874432 -> 139174883252912
	139174883252912 [label=AccumulateGrad]
	139174883248976 -> 139174883250416
	139174879388976 [label="bn3.weight
 (16)" fillcolor=lightblue]
	139174879388976 -> 139174883248976
	139174883248976 [label=AccumulateGrad]
	139174883247152 -> 139174883250416
	139174866684336 [label="bn3.bias
 (16)" fillcolor=lightblue]
	139174866684336 -> 139174883247152
	139174883247152 [label=AccumulateGrad]
	139174883249264 -> 139174883249600
	139174883249264 [label=TBackward0]
	139174883248400 -> 139174883249264
	139174866684496 [label="layer4.weight
 (8, 16)" fillcolor=lightblue]
	139174866684496 -> 139174883248400
	139174883248400 [label=AccumulateGrad]
	139174883249840 -> 139174883249408
	139174866684656 [label="bn4.weight
 (8)" fillcolor=lightblue]
	139174866684656 -> 139174883249840
	139174883249840 [label=AccumulateGrad]
	139174883249792 -> 139174883249408
	139174866687136 [label="bn4.bias
 (8)" fillcolor=lightblue]
	139174866687136 -> 139174883249792
	139174883249792 [label=AccumulateGrad]
	139174883249120 -> 139174865228400
	139174883249120 [label=TBackward0]
	139174883250464 -> 139174883249120
	139174866687536 [label="output_layer.weight
 (1, 8)" fillcolor=lightblue]
	139174866687536 -> 139174883250464
	139174883250464 [label=AccumulateGrad]
	139174865236656 -> 139174893431824
}
