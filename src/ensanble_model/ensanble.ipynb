{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import optuna\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc,matthews_corrcoef, precision_recall_curve,roc_auc_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_1(nn.Module):\n",
    "    def __init__(self, input_dim,dropout_rate=0.35):\n",
    "        super(DNN_1, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,64)\n",
    "        self.layer2 = nn.Linear(64,32)\n",
    "        self.layer3 = nn.Linear(32, 16)\n",
    "        self.layer4 = nn.Linear(16, 8)\n",
    "        self.output_layer = nn.Linear(8, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.relu(self.layer4(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_2(nn.Module):\n",
    "    def __init__(self, input_dim,dropout1=0.35,dropout2=0.5,dropout3=0.25,dropout4=0.1):\n",
    "        super(DNN_2, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,64)\n",
    "        self.layer2 = nn.Linear(64,32)\n",
    "        self.layer3 = nn.Linear(32,16)\n",
    "        self.layer4 = nn.Linear(16,8)\n",
    "        self.output_layer = nn.Linear(8, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.dropout3 = nn.Dropout(dropout3)\n",
    "        self.dropout4 = nn.Dropout(dropout4)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = torch.relu(self.layer4(x))\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_3(nn.Module):\n",
    "    def __init__(self, input_dim,dropout1=0.15,dropout2=0.45):\n",
    "        super(DNN_3, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,64)\n",
    "        self.layer2 = nn.Linear(64,32)\n",
    "        self.layer3 = nn.Linear(32, 16)\n",
    "        self.layer4 = nn.Linear(16, 8)\n",
    "        self.output_layer = nn.Linear(8, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = torch.relu(self.layer4(x))\n",
    "    \n",
    "\n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_4(nn.Module):\n",
    "    def __init__(self, input_dim,dropout1=0.0,dropout2=0.1):\n",
    "        super(DNN_4, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,64)\n",
    "        self.layer2 = nn.Linear(64,32)\n",
    "        self.layer3 = nn.Linear(32, 16)\n",
    "        self.layer4 = nn.Linear(16, 8)\n",
    "        self.output_layer = nn.Linear(8, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "    \n",
    "\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        \n",
    "\n",
    "        x = torch.relu(self.layer4(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_5(nn.Module):\n",
    "    def __init__(self, input_dim,dropout_rate=0.2):\n",
    "        super(DNN_5, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,128)\n",
    "        self.layer2 = nn.Linear(128,64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.layer4 = nn.Linear(32, 16)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.relu(self.layer4(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_6(nn.Module):\n",
    "    def __init__(self, input_dim,dropout1=0.45,dropout2=0.35,dropout3=0.3,dropout4=0.3):\n",
    "        super(DNN_6, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,64)\n",
    "        self.layer2 = nn.Linear(64,32)\n",
    "        self.layer3 = nn.Linear(32,16)\n",
    "        self.layer4 = nn.Linear(16,8)\n",
    "        self.output_layer = nn.Linear(8, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.dropout3 = nn.Dropout(dropout3)\n",
    "        self.dropout4 = nn.Dropout(dropout4)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = torch.relu(self.layer4(x))\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_7(nn.Module):\n",
    "    def __init__(self, input_dim,dropout1=0.4,dropout2=0.1):\n",
    "        super(DNN_7, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,64)\n",
    "        self.layer2 = nn.Linear(64,32)\n",
    "        self.layer3 = nn.Linear(32, 16)\n",
    "        self.layer4 = nn.Linear(16, 8)\n",
    "        self.output_layer = nn.Linear(8, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = torch.relu(self.layer4(x))\n",
    "    \n",
    "\n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_8(nn.Module):\n",
    "    def __init__(self, input_dim,dropout1=0.4,dropout2=0.2):\n",
    "        super(DNN_8, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,128)\n",
    "        self.layer2 = nn.Linear(128,64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.layer4 = nn.Linear(32, 16)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "    \n",
    "\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        \n",
    "\n",
    "        x = torch.relu(self.layer4(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_9(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN_9, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,64)\n",
    "        self.layer2 = nn.Linear(64,32)\n",
    "        self.layer3 = nn.Linear(32, 16)\n",
    "        self.layer4 = nn.Linear(16, 8)\n",
    "        self.output_layer = nn.Linear(8, 1)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "\n",
    "        x = torch.relu(self.layer3(x))\n",
    "\n",
    "        x = torch.relu(self.layer4(x))\n",
    "\n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_10(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN_10, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,128)\n",
    "        self.layer2 = nn.Linear(128,64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.layer4 = nn.Linear(32, 16)\n",
    "\n",
    "        self.output_layer=nn.Linear(16,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "        \n",
    "        x = torch.relu(self.layer3(x))\n",
    "        \n",
    "        x = torch.relu(self.layer4(x))\n",
    "\n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_11(nn.Module):\n",
    "    def __init__(self, input_dim,dropout1=0.5,dropout2=0.1):\n",
    "        super(DNN_11, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,64)\n",
    "        self.layer2 = nn.Linear(64,32)\n",
    "        self.layer3 = nn.Linear(32, 16)\n",
    "        self.layer4 = nn.Linear(16, 8)\n",
    "        self.output_layer = nn.Linear(8, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "    \n",
    "\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = torch.relu(self.layer4(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_12(nn.Module):\n",
    "    def __init__(self, input_dim,dropout1=0.2,dropout2=0.25):\n",
    "        super(DNN_12, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,64)\n",
    "        self.layer2 = nn.Linear(64,32)\n",
    "        self.layer3 = nn.Linear(32, 16)\n",
    "        self.layer4 = nn.Linear(16, 8)\n",
    "        self.output_layer = nn.Linear(8, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "    \n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        \n",
    "\n",
    "        x = torch.relu(self.layer4(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_13(nn.Module):\n",
    "    def __init__(self, input_dim,dropout1=0.05,dropout2=0.3):\n",
    "        super(DNN_13, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,128)\n",
    "        self.layer2 = nn.Linear(128,64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.layer4 = nn.Linear(32, 16)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "    \n",
    "\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = torch.relu(self.layer4(x))\n",
    "        \n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_14(nn.Module):\n",
    "    def __init__(self, input_dim,dropout1=0.4,dropout2=0.3):\n",
    "        super(DNN_14, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,128)\n",
    "        self.layer2 = nn.Linear(128,64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.layer4 = nn.Linear(32, 16)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "    \n",
    "\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        \n",
    "\n",
    "        x = torch.relu(self.layer4(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../../data/learning_data.csv',index_col=0)\n",
    "\n",
    "X=df.drop(columns='dengue',axis=1).values\n",
    "y=df['dengue'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "\n",
    "X_train_tensor=torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor=torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor=torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor=torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207159/2634408850.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path, map_location=device))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DNN_4:\n\tUnexpected key(s) in state_dict: \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"bn2.num_batches_tracked\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"bn3.num_batches_tracked\", \"bn4.weight\", \"bn4.bias\", \"bn4.running_mean\", \"bn4.running_var\", \"bn4.num_batches_tracked\". \n\tsize mismatch for layer1.weight: copying a param with shape torch.Size([216, 20]) from checkpoint, the shape in current model is torch.Size([64, 23]).\n\tsize mismatch for layer1.bias: copying a param with shape torch.Size([216]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer2.weight: copying a param with shape torch.Size([138, 216]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for layer2.bias: copying a param with shape torch.Size([138]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer3.weight: copying a param with shape torch.Size([80, 138]) from checkpoint, the shape in current model is torch.Size([16, 32]).\n\tsize mismatch for layer3.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer4.weight: copying a param with shape torch.Size([40, 80]) from checkpoint, the shape in current model is torch.Size([8, 16]).\n\tsize mismatch for layer4.bias: copying a param with shape torch.Size([40]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for output_layer.weight: copying a param with shape torch.Size([1, 40]) from checkpoint, the shape in current model is torch.Size([1, 8]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[158], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m model_classes:\n\u001b[1;32m     38\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_classes[i](input_dim\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 39\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(path, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m     40\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     41\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DNN_4:\n\tUnexpected key(s) in state_dict: \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"bn2.weight\", \"bn2.bias\", \"bn2.running_mean\", \"bn2.running_var\", \"bn2.num_batches_tracked\", \"bn3.weight\", \"bn3.bias\", \"bn3.running_mean\", \"bn3.running_var\", \"bn3.num_batches_tracked\", \"bn4.weight\", \"bn4.bias\", \"bn4.running_mean\", \"bn4.running_var\", \"bn4.num_batches_tracked\". \n\tsize mismatch for layer1.weight: copying a param with shape torch.Size([216, 20]) from checkpoint, the shape in current model is torch.Size([64, 23]).\n\tsize mismatch for layer1.bias: copying a param with shape torch.Size([216]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer2.weight: copying a param with shape torch.Size([138, 216]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for layer2.bias: copying a param with shape torch.Size([138]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for layer3.weight: copying a param with shape torch.Size([80, 138]) from checkpoint, the shape in current model is torch.Size([16, 32]).\n\tsize mismatch for layer3.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for layer4.weight: copying a param with shape torch.Size([40, 80]) from checkpoint, the shape in current model is torch.Size([8, 16]).\n\tsize mismatch for layer4.bias: copying a param with shape torch.Size([40]) from checkpoint, the shape in current model is torch.Size([8]).\n\tsize mismatch for output_layer.weight: copying a param with shape torch.Size([1, 40]) from checkpoint, the shape in current model is torch.Size([1, 8])."
     ]
    }
   ],
   "source": [
    "# デバイス設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    \"../../saved_model/DNN_1.pth\",\n",
    "    \"../../saved_model/DNN_2.pth\",\n",
    "    \"../../saved_model/DNN_3.pth\",\n",
    "    \"../../saved_model/DNN_4.pth\",\n",
    "    \"../../saved_model/DNN_5.pth\",\n",
    "    \"../../saved_model/DNN_6.pth\",\n",
    "    \"../../saved_model/DNN_7.pth\",\n",
    "    \"../../saved_model/DNN_8.pth\",\n",
    "    \"../../saved_model/DNN_10.pth\",\n",
    "    \"../../saved_model/DNN_11.pth\",\n",
    "    \"../../saved_model/DNN_12.pth\",\n",
    "    \"../../saved_model/DNN_13.pth\",\n",
    "    \"../../saved_model/DNN_14.pth\"]\n",
    "\n",
    "model_classes = {0: DNN_1,\n",
    "                 1: DNN_2,\n",
    "                 2: DNN_3,\n",
    "                 3: DNN_4,\n",
    "                 4: DNN_5,\n",
    "                 5: DNN_6,\n",
    "                 6: DNN_7,\n",
    "                 7: DNN_8,\n",
    "                 8: DNN_10,\n",
    "                 9: DNN_11,\n",
    "                 10: DNN_12,\n",
    "                 11: DNN_13,\n",
    "                 12: DNN_14}\n",
    "\n",
    "models=[]\n",
    "\n",
    "for i, path in enumerate(model_paths):\n",
    "    if i in model_classes:\n",
    "        model = model_classes[i](input_dim=X_train.shape[1]).to(device)\n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "        model.eval()\n",
    "        models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outputs=[]\n",
    "test_outputs=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        train_output = model(X_train_tensor.to(device))\n",
    "        train_outputs.append(train_output)\n",
    "        \n",
    "        test_output = model(X_test_tensor.to(device))\n",
    "        test_outputs.append(train_output)\n",
    "\n",
    "train_DL_features = torch.stack(train_outputs, dim=0)        \n",
    "test_DL_features = torch.stack(test_outputs, dim=0)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DL_features_np = train_DL_features.cpu().numpy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "normalized_features = scaler.fit_transform(DL_features_np.reshape(-1, DL_features_np.shape[-1]))\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(normalized_features)\n",
    "cluster_features = kmeans_labels.reshape(DL_features_np.shape[0], -1)\n",
    "\n",
    "pca = PCA(n_components=2)  # 2次元に削減\n",
    "pca_features = pca.fit_transform(normalized_features)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_features = tsne.fit_transform(normalized_features)\n",
    "\n",
    "\n",
    "distance_matrix = cdist(normalized_features, normalized_features, metric='euclidean')\n",
    "similarity_features = 1 / (1 + distance_matrix)  # 距離から類似度に変換\n",
    "\n",
    "\n",
    "all_features = np.hstack([normalized_features,      \n",
    "                          cluster_features,        \n",
    "                          pca_features,             \n",
    "                          tsne_features,            \n",
    "                          similarity_features])\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "train_standardized_features = scaler_standard.fit_transform(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DL_features_np = test_DL_features.cpu().numpy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "normalized_features = scaler.fit_transform(test_DL_features_np.reshape(-1, test_DL_features_np.shape[-1]))\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(normalized_features)\n",
    "cluster_features = kmeans_labels.reshape(test_DL_features_np.shape[0], -1)\n",
    "\n",
    "pca = PCA(n_components=2)  # 2次元に削減\n",
    "pca_features = pca.fit_transform(normalized_features)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_features = tsne.fit_transform(normalized_features)\n",
    "\n",
    "\n",
    "distance_matrix = cdist(normalized_features, normalized_features, metric='euclidean')\n",
    "similarity_features = 1 / (1 + distance_matrix)  # 距離から類似度に変換\n",
    "\n",
    "\n",
    "all_features = np.hstack([normalized_features,      \n",
    "                          cluster_features,        \n",
    "                          pca_features,             \n",
    "                          tsne_features,            \n",
    "                          similarity_features])\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "test_standardized_features = scaler_standard.fit_transform(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_standardized_features, y_train, test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor=torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor=torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor=torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor=torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(test_standardized_features, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim,dropout1,dropout2):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,64)\n",
    "        self.layer2 = nn.Linear(64,32)\n",
    "        self.layer3 = nn.Linear(32, 16)\n",
    "        self.layer4 = nn.Linear(16, 8)\n",
    "        self.output_layer = nn.Linear(8, 1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dropout2 = nn.Dropout(dropout2)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        \n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = torch.relu(self.layer4(x))\n",
    "    \n",
    "\n",
    "        x = torch.sigmoid(self.output_layer(x)) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # ハイパーパラメータのサンプリング\n",
    "    dropout1 = trial.suggest_float(\"dropout1\", 0, 0.5, step=0.05)\n",
    "    dropout2 = trial.suggest_float(\"dropout2\", 0, 0.5, step=0.05)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  \n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-2)   \n",
    "\n",
    "    # モデル定義\n",
    "    model = DNN_3(\n",
    "        input_dim=X_train_tensor.shape[1],\n",
    "        dropout1=dropout1,\n",
    "        dropout2=dropout2,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Early Stoppingの設定\n",
    "    patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # 訓練ループ\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # バリデーション評価\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_true, val_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                val_outputs = model(X_val).squeeze()\n",
    "                val_loss += criterion(val_outputs, y_val.squeeze()).item()\n",
    "                predictions = (val_outputs >0.49).float()\n",
    "                val_true.extend(y_val.cpu().numpy())\n",
    "                val_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Early Stopping判定\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        # Optunaへのログ\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # 最終的な評価指標を計算\n",
    "    acc = accuracy_score(val_true, val_pred)\n",
    "    print(f\"Final MCC: {acc:.4f}\")\n",
    "\n",
    "    return acc  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = 0  # 使用するGPUのID（0または1）\n",
    "torch.cuda.set_device(device_id)\n",
    "device = torch.device(f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Optunaの設定\n",
    "study = optuna.create_study(direction=\"maximize\")  \n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# 結果表示\n",
    "print(\"Best Parameters: \", study.best_params)\n",
    "print(\"Best Validation F1: \", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params=study.best_params\n",
    "model = DNN(input_dim=X_train_tensor.shape[1],\n",
    "              dropout1=best_params[\"dropout1\"],\n",
    "              dropout2=best_params[\"dropout2\"],\n",
    "              ).to(device)\n",
    "# 最適化と訓練を実行\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"],weight_decay=best_params[\"weight_decay\"])\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "num_epochs=100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = 0\n",
    "    epoch_val_loss = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    if val_loader is not None:\n",
    "        model.eval()  \n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                val_outputs = model(X_val).squeeze()\n",
    "                val_loss = criterion(val_outputs, y_val.squeeze())\n",
    "                epoch_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        model.train()  \n",
    "\n",
    "    if val_loader is not None:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "if val_losses:\n",
    "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Curve for DNN3')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = X_test_tensor.to(device)\n",
    "    y_test_tensor = y_test_tensor.to(device)\n",
    "\n",
    "    # 予測と確率\n",
    "    test_outputs = model(X_test_tensor).squeeze()\n",
    "    predictions = (test_outputs >0.49).float()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "    y_pred = predictions.cpu().numpy()\n",
    "    y_prob = test_outputs.cpu().numpy()\n",
    "\n",
    "# 評価指標\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    specificity = recall_score(y_true, y_pred, pos_label=0)  \n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'Matthews Correlation Coefficient: {mcc:.4f}')\n",
    "    print(f'Specificity: {specificity:.4f}')\n",
    "\n",
    "    # 混同行列（割合表示）\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2%\", cmap=\"Blues\", cbar=False)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix (Normalized)\")\n",
    "    plt.show()\n",
    "\n",
    "    # ROC曲線とAUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--') \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - Model ')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Precision-Recall曲線\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc = auc(recall_curve, precision_curve)\n",
    "    plt.plot(recall_curve, precision_curve, label=f'PR curve (AUC = {pr_auc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve ')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../../saved_model/DNN.pth')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
