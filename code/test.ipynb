{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dengue import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'preprocess' object has no attribute 'save_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/gonken2019/DengueFeverdetection/code/test.ipynb セル 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gonken2019/DengueFeverdetection/code/test.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test\u001b[39m=\u001b[39mpreprocess()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gonken2019/DengueFeverdetection/code/test.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m text\u001b[39m=\u001b[39mtest\u001b[39m.\u001b[39mOCR(\u001b[39m'\u001b[39m\u001b[39m../1810.04805v2.pdf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gonken2019/DengueFeverdetection/code/test.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test\u001b[39m.\u001b[39;49msave_text(text,\u001b[39m'\u001b[39m\u001b[39mtesrt_file\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'preprocess' object has no attribute 'save_text'"
     ]
    }
   ],
   "source": [
    "test=preprocess()\n",
    "text=test.OCR('../1810.04805v2.pdf')\n",
    "test.save_text(text,'tesrt_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jority class.\n",
      "\n",
      "C_ Additional Ablation Studies\n",
      "C.1 Effect of Number of Training Steps\n",
      "\n",
      "Figure 5 presents MNLI Dev accuracy after fine-\n",
      "tuning from a checkpoint that has been pre-trained\n",
      "for k steps. This allows us to answer the following\n",
      "questions:\n",
      "\n",
      "1. Question: Does BERT really need such\n",
      "a large amount of pre-training (128,000\n",
      "words/batch * 1,000,000 steps) to achieve\n",
      "high fine-tuning accuracy?\n",
      "\n",
      "Answer: Yes, BERTpasp achieves almost\n",
      "1.0% additional accuracy on MNLI when\n",
      "trained on 1M steps compared to 500k steps.\n",
      "\n",
      "2. Question: Does MLM pre-training converge\n",
      "\n",
      "slower than LTR pre-training, since only 15%\n",
      "of words are predicted in each batch rather\n",
      "than every word?\n",
      "Answer: The MLM model does converge\n",
      "slightly slower than the LTR model. How-\n",
      "ever, in terms of absolute accuracy the MLM\n",
      "model begins to outperform the LTR model\n",
      "almost immediately.\n",
      "\n",
      "C.2 Ablation for Different Masking\n",
      "Procedures\n",
      "\n",
      "In Section 3.1, we mention that BERT uses a\n",
      "mixed strategy for masking the target tokens when\n",
      "pre-training with the masked language model\n",
      "(MLM) objective. The following is an ablation\n",
      "study to evaluate the effect of different masking\n",
      "strategies.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "MNLI Dev Accuracy\n",
      "\n",
      "78 |- |\n",
      "\n",
      "A] BERTsZaseE (Masked LM)\n",
      "—< BERTBgasg (Left-to-Right)\n",
      "\n",
      "76 |-\n",
      "\n",
      " \n",
      "\n",
      "200 400 600 800\n",
      "Pre-training Steps (Thousands)\n",
      "\n",
      "1,000\n",
      "\n",
      "Figure 5: Ablation over number of training steps. This\n",
      "shows the MNLI accuracy after fine-tuning, starting\n",
      "from model parameters that have been pre-trained for\n",
      "k steps. The x-axis is the value of k.\n",
      "\n",
      "Note that the purpose of the masking strategies\n",
      "is to reduce the mismatch between pre-training\n",
      "and fine-tuning, as the [MASK] symbol never ap-\n",
      "pears during the fine-tuning stage. We report the\n",
      "Dev results for both MNLI and NER. For NER,\n",
      "we report both fine-tuning and feature-based ap-\n",
      "proaches, as we expect the mismatch will be am-\n",
      "plified for the feature-based approach as the model\n",
      "will not have the chance to adjust the representa-\n",
      "tions.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Masking Rates Dev Set Results\n",
      "MASK SAME RND_ MNII NER\n",
      "Fine-tune Fine-tune Feature-based\n",
      "\n",
      "80% 10% 10% 84.2 95.4 94.9\n",
      "100% 0% 0% 84.3 94.9 94.0\n",
      "80% 0% 20% 84.1 95.2 94.6\n",
      "80% 20% 0% 84.4 95.2 94.7\n",
      "0% 20% 80% 83.7 94.8 94.6\n",
      "0% 0% 100% 83.6 94.9 94.6\n",
      "\n",
      "Table 8: Ablation over different masking strategies.\n",
      "\n",
      "The results are presented in Table 8. In the table,\n",
      "MASK means that we replace the target token with\n",
      "the [MASK] symbol for MLM; SAME means that\n",
      "we keep the target token as is; RND means that\n",
      "we replace the target token with another random\n",
      "token.\n",
      "\n",
      "The numbers in the left part of the table repre-\n",
      "sent the probabilities of the specific strategies used\n",
      "during MLM pre-training (BERT uses 80%, 10%,\n",
      "10%). The right part of the paper represents the\n",
      "Dev set results. For the feature-based approach,\n",
      "we concatenate the last 4 layers of BERT as the\n",
      "features, which was shown to be the best approach\n",
      "in Section 5.3.\n",
      "\n",
      "From the table it can be seen that fine-tuning is\n",
      "surprisingly robust to different masking strategies.\n",
      "However, as expected, using only the MASK strat-\n",
      "egy was problematic when applying the feature-\n",
      "based approach to NER. Interestingly, using only\n",
      "the RND strategy performs much worse than our\n",
      "strategy as well.\n",
      "\f\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jority class.\\n\\nC_ Additional Ablation Studies\\nC.1 Effect of Number of Training Steps\\n\\nFigure 5 presents MNLI Dev accuracy after fine-\\ntuning from a checkpoint that has been pre-trained\\nfor k steps. This allows us to answer the following\\nquestions:\\n\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh fine-tuning accuracy?\\n\\nAnswer: Yes, BERTpasp achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n\\n2. Question: Does MLM pre-training converge\\n\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\n\\nC.2 Ablation for Different Masking\\nProcedures\\n\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n\\n \\n\\n \\n\\n \\n\\nMNLI Dev Accuracy\\n\\n78 |- |\\n\\nA] BERTsZaseE (Masked LM)\\n—< BERTBgasg (Left-to-Right)\\n\\n76 |-\\n\\n \\n\\n200 400 600 800\\nPre-training Steps (Thousands)\\n\\n1,000\\n\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after fine-tuning, starting\\nfrom model parameters that have been pre-trained for\\nk steps. The x-axis is the value of k.\\n\\nNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand fine-tuning, as the [MASK] symbol never ap-\\npears during the fine-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both fine-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\nplified for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\n\\n \\n\\n \\n\\nMasking Rates Dev Set Results\\nMASK SAME RND_ MNII NER\\nFine-tune Fine-tune Feature-based\\n\\n80% 10% 10% 84.2 95.4 94.9\\n100% 0% 0% 84.3 94.9 94.0\\n80% 0% 20% 84.1 95.2 94.6\\n80% 20% 0% 84.4 95.2 94.7\\n0% 20% 80% 83.7 94.8 94.6\\n0% 0% 100% 83.6 94.9 94.6\\n\\nTable 8: Ablation over different masking strategies.\\n\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; RND means that\\nwe replace the target token with another random\\ntoken.\\n\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the specific strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\n\\nFrom the table it can be seen that fine-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe RND strategy performs much worse than our\\nstrategy as well.\\n\\x0c'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dengue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
